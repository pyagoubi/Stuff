{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUYr7rXfkCaPkBmlLZ5w+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyagoubi/Stuff/blob/main/cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J09Jvf9_QzV",
        "outputId": "9db16950-b0ed-4e28-8094-101041dab4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "x1tSnf8M_Sr5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = ['bakery' , 'bar', 'education', 'grocery', 'kiosk', 'post',\n",
        "               'restaurant', 'veloparking', 'pharmacy', 'public_park',\n",
        "               'coworking', 'haltestelle']\n",
        "\n",
        "#file_names = ['haltestelle']\n",
        "\n",
        "muns_df = pd.read_excel('/content/drive/MyDrive/mun/data/municipalities.xlsx')\n",
        "muns_df.rename(columns={muns_df.columns[0]: \"municipality\"}, inplace=True)\n",
        "\n",
        "# Load gmd dataframe (replace 'gmd.csv' with the actual file or path)\n",
        "gmd_df = pd.read_excel('/content/drive/MyDrive/mun/data/gmd.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "def filter_dataframes(gmd_df, file_names):\n",
        "    save_dir = '/content/drive/MyDrive/mun/data/cleaned_v01/'\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    total_files = len(file_names)\n",
        "    current_file = 0\n",
        "\n",
        "    all_dfs = {}  # Dictionary to hold all the dataframes\n",
        "\n",
        "    for file_name in file_names:\n",
        "        current_file += 1\n",
        "        print(f'Processing file {current_file}/{total_files}: {file_name}...')\n",
        "\n",
        "        df = pd.read_excel(f'/content/drive/MyDrive/mun/data/{file_name}.xlsx')\n",
        "\n",
        "        df = df[df['Address'].astype(str).str.contains('Switzerland', case=False)]\n",
        "        df.drop_duplicates(subset=['Name', 'Address'], keep='first', inplace=True)\n",
        "\n",
        "        clean_df = pd.DataFrame()\n",
        "        dirty_df = pd.DataFrame()\n",
        "\n",
        "        dirty_df = df  # Since we're not checking municipalities in address, all rows are considered 'dirty' initially\n",
        "\n",
        "        drop_indices = []\n",
        "\n",
        "        for i, row in dirty_df.iterrows():\n",
        "            for j, gmd_row in gmd_df.iterrows():\n",
        "                if str(gmd_row['PLZ']) in row['Address']:\n",
        "                    dirty_df.at[i, 'Municipality'] = gmd_row['Gemeindename']\n",
        "                    clean_df = pd.concat([clean_df, pd.DataFrame(dirty_df.loc[i]).T], ignore_index=True)\n",
        "                    drop_indices.append(i)\n",
        "\n",
        "        dirty_df.drop(drop_indices, inplace=True)\n",
        "\n",
        "        clean_df.to_excel(save_dir + file_name + '_clean.xlsx', index=False)\n",
        "        dirty_df.to_excel(save_dir + file_name + '_dirty.xlsx', index=False)\n",
        "\n",
        "        all_dfs[file_name + '_clean'] = clean_df\n",
        "        all_dfs[file_name + '_dirty'] = dirty_df\n",
        "\n",
        "        print(f'Finished processing file {current_file}/{total_files}: {file_name}. Saved the cleaned and dirty versions.')\n",
        "\n",
        "    return all_dfs\n",
        "\n",
        "# Run the function and store the dictionary\n",
        "all_dataframes = filter_dataframes(gmd_df, file_names)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LPQs1aIsYg93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# The folder where the clean DataFrames are stored\n",
        "old_dir = '/content/drive/MyDrive/mun/data/cleaned_v01/'\n",
        "\n",
        "new_dir = '/content/drive/MyDrive/mun/data/clean_v03/'\n",
        "\n",
        "# Create new directory to store cleaned files\n",
        "if not os.path.exists(new_dir):\n",
        "    os.makedirs(new_dir)\n",
        "\n",
        "# List of file names\n",
        "file_names = ['bakery' , 'bar', 'education', 'grocery', 'kiosk', 'post',\n",
        "               'restaurant', 'veloparking', 'pharmacy', 'public_park',\n",
        "               'coworking', 'haltestelle']\n",
        "\n",
        "# Loop over all files\n",
        "for file_name in file_names:\n",
        "    print(f'Processing file {file_name}...')\n",
        "\n",
        "    # Load the clean DataFrame\n",
        "    clean_df = pd.read_excel(old_dir + file_name + '_cleanv02.xlsx')\n",
        "\n",
        "    # Remove rows that are duplicated in 'Name' and 'Address', keeping the last one (assuming the correct municipality is the last one)\n",
        "    clean_df.drop_duplicates(subset=['Name', 'Address'], keep='first', inplace=True)\n",
        "\n",
        "    # Save the clean DataFrame\n",
        "    clean_df.to_excel(new_dir + file_name + '_clean_v03.xlsx', index=False)\n",
        "\n",
        "    print(f'Finished processing file {file_name}. Saved the cleaned version.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHFW8CWyzwa",
        "outputId": "e7019a65-7f81-4659-e123-3dcf27123a5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file bakery...\n",
            "Finished processing file bakery. Saved the cleaned version.\n",
            "Processing file bar...\n",
            "Finished processing file bar. Saved the cleaned version.\n",
            "Processing file education...\n",
            "Finished processing file education. Saved the cleaned version.\n",
            "Processing file grocery...\n",
            "Finished processing file grocery. Saved the cleaned version.\n",
            "Processing file kiosk...\n",
            "Finished processing file kiosk. Saved the cleaned version.\n",
            "Processing file post...\n",
            "Finished processing file post. Saved the cleaned version.\n",
            "Processing file restaurant...\n",
            "Finished processing file restaurant. Saved the cleaned version.\n",
            "Processing file veloparking...\n",
            "Finished processing file veloparking. Saved the cleaned version.\n",
            "Processing file pharmacy...\n",
            "Finished processing file pharmacy. Saved the cleaned version.\n",
            "Processing file public_park...\n",
            "Finished processing file public_park. Saved the cleaned version.\n",
            "Processing file coworking...\n",
            "Finished processing file coworking. Saved the cleaned version.\n",
            "Processing file haltestelle...\n",
            "Finished processing file haltestelle. Saved the cleaned version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n"
      ],
      "metadata": {
        "id": "mK3k0xSRyzpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def combine_excels_in_folder_to_tabs(input_folder, output_filename):\n",
        "    writer = pd.ExcelWriter(output_filename, engine='openpyxl')\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith('.xlsx'):  # Ensure we're dealing with an .xlsx file\n",
        "            df = pd.read_excel(os.path.join(input_folder, filename))\n",
        "            # Split the filename at '_clean', take the first part, and use it as the sheet name\n",
        "            tab_name = filename.split('_clean')[0]\n",
        "            df.to_excel(writer, sheet_name=tab_name, index=False)\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "# usage\n",
        "combine_excels_in_folder_to_tabs('/content/drive/MyDrive/mun/data/clean_v03/', '/content/drive/MyDrive/mun/data/combinedv03.xlsx')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AVZ74SG6BFM",
        "outputId": "1d8374cd-d9dd-45dd-a2e9-726599e2f830"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-c7da7292aeab>:18: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
            "  writer.save()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfWAj5NP6BCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P5x1u4Nw6A_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaQnETwD6A9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kledyfNP6A6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3d0RiJ_J1olr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def further_clean_all(muns_df, dir_path):\n",
        "    all_dfs = {}  # Dictionary to hold all the dataframes\n",
        "\n",
        "    # Load all '_dirty' Excel files from the directory\n",
        "    all_dirty_files = glob.glob(f\"{dir_path}/*_dirty.xlsx\")\n",
        "\n",
        "    for file_path in all_dirty_files:\n",
        "        # Extract the file name without '_dirty.xlsx'\n",
        "        file_name = file_path.split('/')[-1].replace('_dirty.xlsx', '')\n",
        "\n",
        "        # Load the dirty dataframe from the file\n",
        "        dirty_df = pd.read_excel(file_path)\n",
        "        # Load the clean dataframe from the file\n",
        "        clean_df = pd.read_excel(f\"{dir_path}/{file_name}_clean.xlsx\")\n",
        "\n",
        "        drop_indices = []\n",
        "\n",
        "        for i, row in dirty_df.iterrows():\n",
        "            for municipality in muns_df['municipality']:\n",
        "                # Check if the municipality is in the 'Address' as a standalone word\n",
        "                # We use word boundaries (\\b) in regex to ensure that we match the municipality as a standalone word\n",
        "                if re.search(fr'\\b{municipality}\\b', row['Address']):\n",
        "                    dirty_df.at[i, 'Municipality'] = municipality  # Replace 'Municipality' field with the matching municipality\n",
        "                    clean_df = pd.concat([clean_df, pd.DataFrame(dirty_df.loc[i]).T], ignore_index=True)  # Add row to clean_df\n",
        "                    drop_indices.append(i)  # Add the index to the list of rows to be dropped\n",
        "                    break  # Exit the loop once we find a match\n",
        "\n",
        "        # Drop the matched rows from dirty_df\n",
        "        dirty_df.drop(drop_indices, inplace=True)\n",
        "\n",
        "        # Save the updated clean dataframe and the remaining dirty dataframe back to disk\n",
        "        clean_df.to_excel(f\"{dir_path}/{file_name}_cleanv02.xlsx\", index=False)\n",
        "        dirty_df.to_excel(f\"{dir_path}/{file_name}_dirtyv02.xlsx\", index=False)\n",
        "\n",
        "        # Add the updated clean dataframe and the remaining dirty dataframe to the dictionary\n",
        "        all_dfs[file_name + '_cleanv02'] = clean_df\n",
        "        all_dfs[file_name + '_dirtyv02'] = dirty_df\n",
        "\n",
        "    return all_dfs\n",
        "\n",
        "# Run the function\n",
        "dir_path = \"/content/drive/MyDrive/mun/data/cleaned_v01\"  # Replace with your directory path\n",
        "all_dfs = further_clean_all(muns_df, dir_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "puJgwbbE1oi4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEh2Vszl1ofC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jU5Gwo711oVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_excel_files(file_names, directory):\n",
        "    # Initialize two dictionaries to store dataframes\n",
        "    clean_dfs = {}\n",
        "    dirty_dfs = {}\n",
        "\n",
        "    for file_name in file_names:\n",
        "        # Construct file paths for clean and dirty data\n",
        "        clean_file_path = os.path.join(directory, f\"{file_name}_clean.xlsx\")\n",
        "        dirty_file_path = os.path.join(directory, f\"{file_name}_dirty.xlsx\")\n",
        "\n",
        "        # Load the excel files into pandas DataFrames\n",
        "        if os.path.exists(clean_file_path):\n",
        "            clean_dfs[file_name] = pd.read_excel(clean_file_path)\n",
        "        else:\n",
        "            print(f\"File not found: {clean_file_path}\")\n",
        "\n",
        "        if os.path.exists(dirty_file_path):\n",
        "            dirty_dfs[file_name] = pd.read_excel(dirty_file_path)\n",
        "        else:\n",
        "            print(f\"File not found: {dirty_file_path}\")\n",
        "\n",
        "    return clean_dfs, dirty_dfs\n",
        "\n",
        "clean_dfs, dirty_dfs = load_excel_files(file_names, '/content/drive/MyDrive/mun/data/cleaned_v01')\n"
      ],
      "metadata": {
        "id": "NahvGtLRZ7ZM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataframe_lengths(dfs):\n",
        "    for name, df in dfs.items():\n",
        "        print(f\"Length of {name}: {len(df)}\")\n",
        "\n",
        "print_dataframe_lengths(dirty_dfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61cUUKY9Vm69",
        "outputId": "5bcd9f99-a6a7-4688-faa4-d20fc6916aa9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of bakery: 17\n",
            "Length of bar: 45\n",
            "Length of education: 96\n",
            "Length of grocery: 8\n",
            "Length of kiosk: 53\n",
            "Length of post: 12\n",
            "Length of restaurant: 26\n",
            "Length of veloparking: 4\n",
            "Length of pharmacy: 21\n",
            "Length of public_park: 25\n",
            "Length of coworking: 2\n",
            "Length of haltestelle: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_excel_files(file_names, directory, output_file):\n",
        "    with pd.ExcelWriter(output_file) as writer:\n",
        "        for file_name in file_names:\n",
        "            # Load the clean and dirty dataframes\n",
        "            clean_df = pd.read_excel(f\"{directory}/{file_name}_clean.xlsx\")\n",
        "            dirty_df = pd.read_excel(f\"{directory}/{file_name}_dirty.xlsx\")\n",
        "\n",
        "            # Save them into the excel file with their respective names as the sheet name\n",
        "            clean_df.to_excel(writer, sheet_name=f\"{file_name}_clean\", index=False)\n",
        "            dirty_df.to_excel(writer, sheet_name=f\"{file_name}_dirty\", index=False)\n",
        "\n",
        "# Call the function\n",
        "combine_excel_files(file_names, '/content/drive/MyDrive/mun/data/cleaned_v01', '/content/drive/MyDrive/mun/combined.xlsx')\n"
      ],
      "metadata": {
        "id": "wqPnC9x1Vm4F"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SvBvPgIQVm1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIs_JQP1Vmyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iezU2jrNVmvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zi-vTMI2Vmr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yt30BoHBZ7WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-IG9vfbAmu_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KtJ2hr5mu8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abObAHk-mu6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVsSd9yomu3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrTdF7U3mu0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLy5gJHRmuxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2x_bPw8lMO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}