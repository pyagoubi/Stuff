{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtMVrPxLfalzoL23A4Fznn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyagoubi/Stuff/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q91fSnZuQgn2",
        "outputId": "e4cdc160-2d63-43db-b34a-fc5f75937ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install backtesting\n",
        "\n",
        "import backtesting as bt\n",
        "from backtesting import Backtest, Strategy"
      ],
      "metadata": {
        "id": "8R6JWeuGRI4B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# set cpu or gpu enabled device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type"
      ],
      "metadata": {
        "id": "DVF3UKTFRQlz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create LSTM model for 1 day forecasting"
      ],
      "metadata": {
        "id": "PfjAAAwER0EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cfg_LSTM1d:  \n",
        "  split_fraction = 0.85\n",
        "  time_steps = 5 # number of predictor timesteps\n",
        "  horizon = 1 # number of timesteps to be predicted\n",
        "  sequence_length = time_steps + horizon # determine sequence length\n",
        "  learning_rate=0.01\n",
        "  num_epochs=2000\n",
        "  path = '/content/drive/MyDrive/stock predict/technical/1D_technical.csv'\n",
        "  features = ['close']\n",
        "  target = 'close'\n",
        "  input_size = len(features)\n",
        "  hidden_size = 50\n",
        "  output_size = 1\n",
        "  numlayers = 2\n",
        "  save_path = '/content/drive/MyDrive/stock predict/pred_close1d.pt'\n",
        "\n",
        "\n",
        "def load_1ddata(path = cfg_LSTM1d.path, features = cfg_LSTM1d.features):\n",
        "  data_1draw = pd.read_csv(path)\n",
        "  data_1draw.columns = data_1draw.columns.str.replace(' ', '')\n",
        "  data_1draw['time'] = pd.to_datetime(data_1draw['time'])\n",
        "  data_1draw['time'] = data_1draw['time'].dt.date\n",
        "  data_1draw.set_index('time', inplace=True)\n",
        "  df_1d = data_1draw[features].copy()\n",
        "  return df_1d, data_1draw\n",
        "\n",
        "\n",
        "def trainvalidsplit(df_1d, split_fraction = cfg_LSTM1d.split_fraction):\n",
        "  split_row = int(df_1d.shape[0] * split_fraction)\n",
        "  train_1d = df_1d.iloc[:split_row].copy()\n",
        "  valid_1d = df_1d.iloc[split_row:].copy()\n",
        "  return train_1d, valid_1d\n",
        "\n",
        "def scale1d(train_1d, valid_1d,f_scaler1d, \n",
        "            t_scaler1d, input_size = cfg_LSTM1d.input_size, target = cfg_LSTM1d.target ):\n",
        "  \n",
        "  train_1d_scaled = train_1d.copy()\n",
        "  valid_1d_scaled = valid_1d.copy()  \n",
        "  \n",
        "  if input_size == 2:\n",
        "    train_1d_scaled.loc[:, train_1d.columns != target] = f_scaler1d.fit_transform(train_1d_scaled.loc[:, train_1d_scaled.columns != target].values.reshape(-1,1))\n",
        "    valid_1d_scaled.loc[:, valid_1d.columns != target] = f_scaler1d.transform(valid_1d.loc[:, valid_1d_scaled.columns != target].values.reshape(-1,1))\n",
        "  elif input_size >2:\n",
        "    train_1d_scaled.loc[:, train_1d.columns != target] = f_scaler1d.fit_transform(train_1d_scaled.loc[:, train_1d_scaled.columns != target])\n",
        "    valid_1d_scaled.loc[:, valid_1d.columns != target] = f_scaler1d.fit_transform(valid_1d_scaled.loc[:, valid_1d_scaled.columns != target])\n",
        "\n",
        "  train_1d_scaled[target] = t_scaler1d.fit_transform(train_1d[target].values.reshape(-1,1))\n",
        "  valid_1d_scaled[target] = t_scaler1d.transform(valid_1d[target].values.reshape(-1,1))\n",
        "  return train_1d_scaled, valid_1d_scaled\n",
        "\n",
        "def create_sequences(df, seq_length = cfg_LSTM1d.sequence_length):\n",
        "    df = df.values  # Convert DataFrame to numpy array\n",
        "    \n",
        "    n = df.shape[0]\n",
        "    xs = np.zeros((n - seq_length, seq_length, df.shape[1]))\n",
        "    ys = np.zeros((n - seq_length, 1))\n",
        "    \n",
        "    for i in range(n - seq_length):\n",
        "        xs[i] = df[i:(i+seq_length)]\n",
        "        ys[i] = df[i+seq_length, -1]  # predict the 'return' column one step ahead\n",
        "    \n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.from_numpy(xs)\n",
        "    y = torch.from_numpy(ys)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size = cfg_LSTM1d.input_size, hidden_size = cfg_LSTM1d.hidden_size, \n",
        "                 num_layers = cfg_LSTM1d.numlayers, output_size=cfg_LSTM1d.output_size):\n",
        "\n",
        "      super(LSTM, self).__init__()\n",
        "      \n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers     \n",
        "      self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout = 0.2)    \n",
        "      self.fc1 = nn.Linear(hidden_size, output_size)\n",
        "      #self.fc2 = nn.Linear(10, output_size)  # Add a second layer\n",
        "      self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "      h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
        "      c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
        "      h0.requires_grad = True\n",
        "      c0.requires_grad = True\n",
        "      out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "      out = self.tanh(self.fc1(out[:, -1, :]))  # apply tanh activation function to the output of the first linear layer\n",
        "      #out = self.fc2(out)  # pass through the second linear layer\n",
        "      return out\n",
        "\n",
        "\n",
        "def train1d(model, train, train_target, valid, valid_target, \n",
        "          learning_rate = cfg_LSTM1d.learning_rate, num_epochs = cfg_LSTM1d.num_epochs, save_path = cfg_LSTM1d.save_path):\n",
        "\n",
        "  criterion = torch.nn.MSELoss(reduction='mean')\n",
        "  optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  train = train.float().to(device)  # Convert to float\n",
        "  train_target = train_target.float().to(device)  # Convert to float\n",
        "  valid = valid.float().to(device)  # Convert to float\n",
        "  valid_target = valid_target.float().to(device)  # Convert to float\n",
        "\n",
        "  best_loss = float('inf')\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    y_train_pred = model(train)\n",
        "\n",
        "    print(y_train_pred.shape)\n",
        "    train_loss = criterion(y_train_pred, train_target)\n",
        "    optimiser.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    model.eval()\n",
        "    outputs = model(valid)\n",
        "    val_loss = criterion(outputs, valid_target)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "              f'Train Loss: {train_loss.item():.4f}, '\n",
        "              f'Validation Loss: {val_loss.item():.4f}')\n",
        "    \n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      torch.save(model, save_path)"
      ],
      "metadata": {
        "id": "bGnWvgmdR2I4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_scaler1d = MinMaxScaler(feature_range=(-1, 1))\n",
        "t_scaler1d = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "df_1d, data_1draw = load_1ddata()\n",
        "train_1d, valid_1d = trainvalidsplit(df_1d)\n",
        "train_1d_scaled, valid_1d_scaled = scale1d(train_1d, valid_1d,f_scaler1d, t_scaler1d)\n",
        "train_sequences1d, train_target1d = create_sequences(train_1d_scaled)\n",
        "valid_sequences1d, valid_target1d = create_sequences(valid_1d_scaled)\n",
        "\n",
        "model1d = LSTM()\n",
        "model1d = model1d.to(device)\n",
        "\n",
        "train1d(model1d, train_sequences1d, train_target1d, valid_sequences1d, valid_target1d)\n"
      ],
      "metadata": {
        "id": "dA0TrTx2VbaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}